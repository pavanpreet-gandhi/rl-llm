\begin{thebibliography}{27}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman, Herzog, Ho, Hsu, Ibarz, Ichter, Irpan, Jang, Jauregui~Ruano, Jeffrey, Jesmonth, Joshi, Julian, Kalashnikov, Kuang, Lee, Levine, Lu, Luu, Parada, Pastor, Quiambao, Rao, Rettinghouse, Reyes, Sermanet, Sievers, Tan, Toshev, Vanhoucke, Xia, Xiao, Xu, Xu, Yan, and Zeng}]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui~Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock In \emph{arXiv preprint arXiv:2204.01691}.

\bibitem[{Carta et~al.(2023)Carta, Romac, Wolf, Lamprier, Sigaud, and Oudeyer}]{carta2023grounding}
Thomas Carta, Cl√©ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. 2023.
\newblock Grounding large language models in interactive environments with online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2302.02662}.

\bibitem[{Chen et~al.(2025)Chen, Cusumano-Towner, Huval, Petrenko, Hamburger, Koltun, and Kr{\"a}henb{\"u}hl}]{chen2025reinforcement}
Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, and Philipp Kr{\"a}henb{\"u}hl. 2025.
\newblock Reinforcement learning for long-horizon interactive llm agents.
\newblock \emph{arXiv preprint arXiv:2502.01600}.

\bibitem[{Chevalier-Boisvert et~al.(2018)Chevalier-Boisvert, Bahdanau, Lahlou, Willems, Saharia, Nguyen, and Bengio}]{chevalier2018babyai}
Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio. 2018.
\newblock Babyai: A platform to study the sample efficiency of grounded language learning.
\newblock \emph{arXiv preprint arXiv:1810.08272}.

\bibitem[{DeepSeek-AI et~al.(2025)DeepSeek-AI, Guo, Yang, Zhang et~al.}]{deepseek2025r1}
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, et~al. 2025.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2501.12948}.

\bibitem[{Gandhi et~al.(2025)Gandhi, Chakravarthy, Singh, Lile, and Goodman}]{gandhi2025cognitive}
Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah~D Goodman. 2025.
\newblock Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars.
\newblock \emph{arXiv preprint arXiv:2503.01307}.

\bibitem[{Gandhi et~al.(2024)Gandhi, Lee, Grand, Liu, Cheng, Sharma, and Goodman}]{gandhi2024stream}
Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah~D Goodman. 2024.
\newblock Stream of search (sos): Learning to search in language.
\newblock \emph{arXiv preprint arXiv:2404.03683}.

\bibitem[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.
\newblock \href {https://doi.org/10.48550/arXiv.2106.09685} {Lora: Low-rank adaptation of large language models}.
\newblock \emph{arXiv preprint arXiv:2106.09685}.

\bibitem[{Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch}]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock \emph{arXiv preprint arXiv:2201.07207}.

\bibitem[{Huang et~al.(2023)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar, Sermanet, Brown, Jackson, Luu, Levine, Hausman, and Ichter}]{huang2022inner}
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2023.
\newblock Inner monologue: Embodied reasoning through planning with language models.
\newblock In \emph{Proceedings of the 6th Conference on Robot Learning}, volume 205, pages 297--307.

\bibitem[{Kazemnejad et~al.(2024)Kazemnejad, Aghajohari, Portelance, Sordoni, Reddy, Courville, and Roux}]{kazemnejad2024vineppo}
Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas~Le Roux. 2024.
\newblock Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment.
\newblock \emph{arXiv preprint arXiv:2410.01679}.

\bibitem[{Kim et~al.(2023)Kim, Baldi, and McAleer}]{kim2023language}
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023.
\newblock Language models can solve computer tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:39648--39677.

\bibitem[{Lambert et~al.(2024)Lambert, Morrison, Pyatkin, Huang, Ivison, Brahman, Miranda, Liu, Dziri, Lyu et~al.}]{lambert2024t}
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James~V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et~al. 2024.
\newblock T$\backslash$" ulu 3: Pushing frontiers in open language model post-training.
\newblock \emph{arXiv preprint arXiv:2411.15124}.

\bibitem[{Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena}]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe}]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}.

\bibitem[{Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Hambro, Zettlemoyer, Cancedda, and Scialom}]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:68539--68551.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}.

\bibitem[{Shinn et~al.(2023)Shinn, Cassano, Gopinath, Narasimhan, and Yao}]{shinn2023reflexion}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:8634--8652.

\bibitem[{Szot et~al.(2024)Szot, Mazoure, Agrawal, Hjelm, Kira, and Toshev}]{szot2024grounding}
Andrew Szot, Bogdan Mazoure, Harsh Agrawal, R~Devon Hjelm, Zsolt Kira, and Alexander Toshev. 2024.
\newblock Grounding multimodal large language models in actions.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:20198--20224.

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar}]{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023{\natexlab{a}}.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V. Le, Ed~H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023{\natexlab{b}}.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~H. Chi, Quoc~V. Le, and Denny Zhou. 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 24824--24837.

\bibitem[{Xiang et~al.(2023)Xiang, Tao, Gu, Shu, Wang, Yang, and Hu}]{xiang2023language}
Jiannan Xiang, Tianhua Tao, Yi~Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. 2023.
\newblock Language models meet world models: Embodied experiences enhance language models.
\newblock \emph{Advances in neural information processing systems}, 36:75392--75412.

\bibitem[{Xiang et~al.(2025)Xiang, Snell, Gandhi, Albalak, Singh, Blagden, Phung, Rafailov, Lile, Mahan et~al.}]{xiang2025towards}
Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et~al. 2025.
\newblock Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though.
\newblock \emph{arXiv preprint arXiv:2501.04682}.

\bibitem[{Yao et~al.(2023{\natexlab{a}})Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan}]{yao2023treethoughtsdeliberateproblem}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2305.10601} {Tree of thoughts: Deliberate problem solving with large language models}.

\bibitem[{Yao et~al.(2023{\natexlab{b}})Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao}]{yao2023react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023{\natexlab{b}}.
\newblock {ReAct}: Synergizing reasoning and acting in language models.
\newblock In \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[{Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman}]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:15476--15488.

\end{thebibliography}
