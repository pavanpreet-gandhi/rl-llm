{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: /workspace/rl-llm\n",
      "Current directory: /workspace/rl-llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if we're in the root directory of rl-llm repo\n",
    "if not os.path.basename(os.getcwd()) == 'rl-llm':\n",
    "    # If we're in a subdirectory of rl-llm, find the root and cd to it\n",
    "    current_path = os.getcwd()\n",
    "    while os.path.basename(current_path) != 'rl-llm' and os.path.dirname(current_path) != current_path:\n",
    "        current_path = os.path.dirname(current_path)\n",
    "    \n",
    "    if os.path.basename(current_path) == 'rl-llm':\n",
    "        os.chdir(current_path)\n",
    "        print(f\"Changed directory to: {current_path}\")\n",
    "    else:\n",
    "        print(\"Not in rl-llm repository structure\")\n",
    "else:\n",
    "    print(\"Already in rl-llm root directory\")\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from src import EnvManager, sample_episodes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Suppress spammy logs from transformers, \n",
    "# e.g. \"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\"\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id, revision=None):\n",
    "    \"\"\"\n",
    "    Load a model with both PEFT adapters and value head\n",
    "    \n",
    "    Args:\n",
    "        model_id: Path to model or HF hub model ID\n",
    "        device: Device to load the model to\n",
    "        revision: Specific model revision/commit hash to load\n",
    "    \n",
    "    Returns:\n",
    "        model: The loaded model with adapters and value head\n",
    "        tokenizer: The associated tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left', revision=revision)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    peft_config = PeftConfig.from_pretrained(model_id, revision=revision)\n",
    "    base_model_id = peft_config.base_model_name_or_path\n",
    "    \n",
    "    logging.info(f\"Loading base model: {base_model_id}\")\n",
    "    logging.info(f\"Loading with PEFT adapters from: {model_id}\")\n",
    "    if revision:\n",
    "        logging.info(f\"Using revision: {revision}\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "    model = PeftModel.from_pretrained(base_model, model_id, revision=revision)\n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_base_model(model_id):\n",
    "    \"\"\"\n",
    "    Load a base model with value head\n",
    "    \n",
    "    Args:\n",
    "        model_id: Path to model or HF hub model ID\n",
    "    \n",
    "    Returns:\n",
    "        model: The loaded model with value head\n",
    "        tokenizer: The associated tokenizer\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    logging.info(f\"Loading base model with value head from: {model_id}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models to be evaluated\n",
    "model_configs = {\n",
    "    \"Baseline\": {\n",
    "        \"load_fn\": load_base_model,\n",
    "        \"model_id\": \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "        \"reasoning_flag\": None\n",
    "    },\n",
    "    \"Trial2r\": {\n",
    "        \"load_fn\": load_model,\n",
    "        \"model_id\": \"CatkinChen/final_runs-Reasoning_trial_2_dist_0\",\n",
    "        \"revision\": '15e9cb622708f56f7da77421420bf42fe426b9c7',   # Optionally set a revision if needed\n",
    "        \"reasoning_flag\": True\n",
    "    },\n",
    "    \"Trial2\": {\n",
    "        \"load_fn\": load_model,\n",
    "        \"model_id\": \"CatkinChen/final_runs-No_Reasoning__trial_2\",\n",
    "        \"revision\": 'fc13267d7ef0c56e840b76728539f75352796456',   # Optionally set a revision if needed\n",
    "        \"reasoning_flag\": False\n",
    "    },\n",
    "    \"Trial1\": {\n",
    "        \"load_fn\": load_model,\n",
    "        \"model_id\": \"Heisenger/final_runs-No_Reasoning_trial_1_dist_0\",\n",
    "        \"revision\": '06aacec58e5a5dc4838810e4f2e480ea293a7e2b',   # Optionally set a revision if needed\n",
    "        \"reasoning_flag\": False\n",
    "    },\n",
    "    \"Trial1r\": {\n",
    "        \"load_fn\": load_model,\n",
    "        \"model_id\": \"Heisenger/final_runs-Reasoning__trial_1\",\n",
    "        \"revision\": '43b59fb7065b4827c066e6295f4bd2b6c1375bf5',   # Optionally set a revision if needed\n",
    "        \"reasoning_flag\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and testing model: Baseline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf031c1d8cb41dfbc3d5e93b57dcdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      "You are in a room with a key. The instruction is to: pick up the key and put it in the locked box. But the box is not locked. So, what can you do?\n",
      "\n",
      "You can put the key in your pocket or on the table.\n",
      "\n",
      "You can walk out of the room.\n",
      "\n",
      "You can close the door.\n",
      "\n",
      "You\n",
      "\n",
      "Loading and testing model: Trial2r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba61b4406e042308accc8953898758f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m load_fn(model_id)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Simple query to test the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are in a room with a key. The instruction is to: pick up the key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_id, revision)\u001b[0m\n\u001b[1;32m     27\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_id)\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, model_id, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[0;32m/workspace/rl-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/rl-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/rl-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/workspace/rl-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/rl-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:965\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    962\u001b[0m     out_param \u001b[38;5;241m=\u001b[39m Parameter(param_applied, param\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters[key] \u001b[38;5;241m=\u001b[39m out_param\n\u001b[0;32m--> 965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    967\u001b[0m         grad_applied \u001b[38;5;241m=\u001b[39m fn(param_grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Iterate over each model configuration to load and test with a simple query\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\nLoading and testing model: {model_name}\")\n",
    "\n",
    "    # Retrieve the loading function, model id, and optionally, the commit hash\n",
    "    load_fn = config[\"load_fn\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    revision = config.get(\"revision\", None)\n",
    "\n",
    "    # Load the model and associated tokenizer using the specified function\n",
    "    if load_fn.__name__ == \"load_base_model\":\n",
    "        model, tokenizer = load_fn(model_id)\n",
    "    else:\n",
    "        model, tokenizer = load_fn(model_id, revision)\n",
    "\n",
    "    # Simple query to test the model\n",
    "    prompt = \"You are in a room with a key. The instruction is to: pick up the key\"\n",
    "    \n",
    "    # Tokenize the input prompt and send tensors to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate output with your text-generation settings\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "\n",
    "    # Decode the generated tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Display the model's output\n",
    "    print(\"Model output:\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seen and unseen environments\n",
    "seen_env_ids = [\"BabyAI-GoTo-v0\", \"BabyAI-Pickup-v0\"]\n",
    "unseen_env_ids = [\"BabyAI-GoToTest-v0\", \"BabyAI-PickupTest-v0\"] #\"BabyAI-Open-v0\", \"BabyAI-PickUpSeqGoTo-v0\"\n",
    "\n",
    "# Number of parallel environments to instantiate for each configuration.\n",
    "num_envs = 6\n",
    "\n",
    "# Initialise a container for the aggregated results.\n",
    "evaluation_results = {}\n",
    "\n",
    "# Function that evaluates a model on a given environment and reasoning flag\n",
    "def evaluate_model_on_envs(env_id, reasoning_flag, model, tokenizer, generation_kwargs, device, num_episodes=50):\n",
    "    if generation_kwargs is None:\n",
    "        generation_kwargs = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.7,\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        }\n",
    "    # Create environment managers for the current env_id and reasoning flag.\n",
    "    env_managers = [\n",
    "        EnvManager(\n",
    "            env_ids=[env_id],\n",
    "            invalid_action_penalty=-1,\n",
    "            consecutive_invalid_actions_allowed=5,\n",
    "            reasoning_flag=reasoning_flag,\n",
    "            num_dists=0,\n",
    "        )\n",
    "        for _ in range(num_envs)\n",
    "    ]\n",
    "    \n",
    "    # Run episodes using sample_episodes\n",
    "    stats, contexts = sample_episodes(\n",
    "        envs=env_managers,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        generation_kwargs=generation_kwargs,\n",
    "        device=device,\n",
    "        number_of_episodes=num_episodes,\n",
    "        context_window=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "    )\n",
    "    \n",
    "    # Calculate the summary metrics.\n",
    "    success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "    avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "    std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "    avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "    std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "    avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "    \n",
    "    # Bundle the results into a dictionary.\n",
    "    results = {\n",
    "        \"success_rate\": success_rate,\n",
    "        \"avg_reward\": avg_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"avg_episode_length\": avg_episode_length,\n",
    "        \"std_episode_length\": std_episode_length,\n",
    "        \"avg_num_invalid_actions\": avg_num_invalid_actions,\n",
    "        \"contexts\": contexts,  # optional: include detailed contexts if needed\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Baseline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8049ffaa82154c41a047c9cfca07e3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline on Seen env: BabyAI-GoTo-v0 with reasoning=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/rl-llm/.venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "/workspace/rl-llm/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "model must be a PreTrainedModelWrapper, got <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> - supported architectures are: (<class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>, <class 'trl.models.modeling_value_head.AutoModelForSeq2SeqLMWithValueHead'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reasoning \u001b[38;5;129;01min\u001b[39;00m reasoning_flags:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_group_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m env: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with reasoning=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_on_envs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreasoning_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_generation_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed.\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Save results with a composite key.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     evaluation_results[(model_name, env_id, reasoning)] \u001b[38;5;241m=\u001b[39m results\n",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m, in \u001b[0;36mevaluate_model_on_envs\u001b[0;34m(env_id, reasoning_flag, model, tokenizer, generation_kwargs, device, num_episodes)\u001b[0m\n\u001b[1;32m     22\u001b[0m env_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m     EnvManager(\n\u001b[1;32m     24\u001b[0m         env_ids\u001b[38;5;241m=\u001b[39m[env_id],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_envs)\n\u001b[1;32m     31\u001b[0m ]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run episodes using sample_episodes\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPOTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m stats, contexts \u001b[38;5;241m=\u001b[39m sample_episodes(\n\u001b[1;32m     36\u001b[0m     envs\u001b[38;5;241m=\u001b[39menv_managers,\n\u001b[1;32m     37\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     reasoning_flag\u001b[38;5;241m=\u001b[39mreasoning_flag,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate the summary metrics.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/rl-llm/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:210\u001b[0m, in \u001b[0;36mPPOTrainer.__init__\u001b[0;34m(self, config, model, ref_model, tokenizer, dataset, optimizer, data_collator, num_shared_layers, lr_scheduler, training_data_collator)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tokenizer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, (SUPPORTED_ARCHITECTURES)):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel must be a PreTrainedModelWrapper, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported architectures are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUPPORTED_ARCHITECTURES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Step 1: Initialize Accelerator\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m Accelerator(\n\u001b[1;32m    215\u001b[0m     log_with\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_with,\n\u001b[1;32m    216\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps,\n\u001b[1;32m    217\u001b[0m     project_config\u001b[38;5;241m=\u001b[39mProjectConfiguration(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproject_kwargs),\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39maccelerator_kwargs,\n\u001b[1;32m    219\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: model must be a PreTrainedModelWrapper, got <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> - supported architectures are: (<class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>, <class 'trl.models.modeling_value_head.AutoModelForSeq2SeqLMWithValueHead'>)"
     ]
    }
   ],
   "source": [
    "# Main Evaluation Loop\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    # Load model and tokenizer via the appropriate function.\n",
    "    load_fn = config[\"load_fn\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    # Load the model using the appropriate function and arguments\n",
    "    if load_fn.__name__ == \"load_base_model\":\n",
    "        model, tokenizer = load_fn(model_id)\n",
    "    else:\n",
    "        revision = config.get(\"revision\")\n",
    "        model, tokenizer = load_fn(model_id, revision)\n",
    "    \n",
    "    # Update generation kwargs for this model. (Make sure you use its pad_token_id.)\n",
    "    local_generation_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.7,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    \n",
    "    reasoning_flag = config[\"reasoning_flag\"]\n",
    "    reasoning_flags = [True, False] if reasoning_flag is None else [reasoning_flag]\n",
    "\n",
    "    # Loop over environments (seen and unseen) and reasoning flags.\n",
    "    # You can separate these groups as needed.\n",
    "    for env_group_name, env_ids in zip([\"Seen\", \"Unseen\"], [seen_env_ids, unseen_env_ids]):\n",
    "        for env_id in env_ids:\n",
    "            for reasoning in reasoning_flags:\n",
    "                print(f\"Evaluating {model_name} on {env_group_name} env: {env_id} with reasoning={reasoning}\")\n",
    "                results = evaluate_model_on_envs(\n",
    "                    env_id=env_id,\n",
    "                    reasoning_flag=reasoning,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    generation_kwargs=local_generation_kwargs,\n",
    "                    device=device,\n",
    "                    num_episodes=10  # Adjust as needed.\n",
    "                )\n",
    "                # Save results with a composite key.\n",
    "                evaluation_results[(model_name, env_id, reasoning)] = results\n",
    "\n",
    "# --- Reporting the results ---\n",
    "for key, result in evaluation_results.items():\n",
    "    model_name, env_id, reasoning = key\n",
    "    print(f\"\\nResults for {model_name} on {env_id} (reasoning={reasoning}):\")\n",
    "    print(f\"  Success Rate: {result['success_rate']}\")\n",
    "    print(f\"  Avg Reward: {result['avg_reward']} (Std: {result['std_reward']})\")\n",
    "    print(f\"  Avg Episode Length: {result['avg_episode_length']} (Std: {result['std_episode_length']})\")\n",
    "    print(f\"  Avg Num. of Invalid Actions: {result['avg_num_invalid_actions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_rates(env_ids, model_configs_to_plot, evaluation_results, title):\n",
    "    \"\"\"\n",
    "    Plot bar chart of success rates for specified model configurations across environments.\n",
    "    \n",
    "    Args:\n",
    "        env_ids (list): List of environment IDs to plot.\n",
    "        model_configs_to_plot (list): List of tuples (model_name, reasoning) to include.\n",
    "        evaluation_results (dict): Dictionary containing evaluation metrics.\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    n_envs = len(env_ids)\n",
    "    n_models = len(model_configs_to_plot)\n",
    "    bar_width = 0.8 / n_models  # Adjust bar width based on number of models\n",
    "    index = np.arange(n_envs)   # Positions for environment groups\n",
    "    \n",
    "    for i, (model_name, reasoning) in enumerate(model_configs_to_plot):\n",
    "        # Extract success rates for this model configuration\n",
    "        success_rates = []\n",
    "        for env in env_ids:\n",
    "            key = (model_name, env, reasoning)\n",
    "            if key in evaluation_results:\n",
    "                success_rates.append(evaluation_results[key][\"success_rate\"])\n",
    "            else:\n",
    "                success_rates.append(0)  # Default to 0 if data is missing\n",
    "        \n",
    "        # Plot bars for this model\n",
    "        label = f\"{model_name} {'R' if reasoning else 'NR'}\"\n",
    "        ax.bar(index + i * bar_width, success_rates, bar_width, label=label)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel(\"Environments\", fontsize=12)\n",
    "    ax.set_ylabel(\"Success Rate\", fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xticks(index + bar_width * (n_models - 1) / 2)\n",
    "    ax.set_xticklabels(env_ids, rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1)  # Success rate ranges from 0 to 1\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define all environments to plot\n",
    "env_ids = seen_env_ids + unseen_env_ids\n",
    "\n",
    "# First plot: Baseline (R), Baseline (NR), Trial1 (NR), Trial1r (R)\n",
    "first_plot_models = [\n",
    "    (\"Baseline\", True),   # Baseline with reasoning\n",
    "    (\"Baseline\", False),  # Baseline without reasoning\n",
    "    (\"Trial1\", False),    # Trial1 without reasoning\n",
    "    (\"Trial1r\", True)     # Trial1r with reasoning\n",
    "]\n",
    "plot_success_rates(\n",
    "    env_ids=env_ids,\n",
    "    model_configs_to_plot=first_plot_models,\n",
    "    evaluation_results=evaluation_results,\n",
    "    title=\"Success Rate Comparison: Baseline and Trial1 Models\"\n",
    ")\n",
    "\n",
    "# Second plot: Baseline (R), Baseline (NR), Trial2 (NR), Trial2r (R)\n",
    "second_plot_models = [\n",
    "    (\"Baseline\", True),   # Baseline with reasoning\n",
    "    (\"Baseline\", False),  # Baseline without reasoning\n",
    "    (\"Trial2\", False),    # Trial2 without reasoning\n",
    "    (\"Trial2r\", True)     # Trial2r with reasoning\n",
    "]\n",
    "plot_success_rates(\n",
    "    env_ids=env_ids,\n",
    "    model_configs_to_plot=second_plot_models,\n",
    "    evaluation_results=evaluation_results,\n",
    "    title=\"Success Rate Comparison: Baseline and Trial2 Models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BabyAI-Pickup-v0\"\n",
    "reasoning_flag = True\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "        num_dists=3,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=50,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BabyAI-PickupTest-v0\"\n",
    "reasoning_flag = True\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "        num_dists=3,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=50,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"CatkinChen/final_runs-No_Reasoning_trial_2_dist_3\"\n",
    "revision = None # \"cf75726c11f5b5867107f56efc8f55439635f1c6\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, tokenizer = load_model_with_peft_and_vhead(model_id, revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_base_model_with_vhead(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "env_id = \"BabyAI-Pickup-v0\"\n",
    "reasoning_flag = True\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "        num_dists=3,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=50,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_base_model_with_vhead(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "env_id = \"BabyAI-Pickup-v0\"\n",
    "reasoning_flag = False\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "        num_dists=3,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=50,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BabyAI-Pickup-v0\"\n",
    "reasoning_flag = False\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "        num_dists=3,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=50,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BabyAI-PickupTest-v0\"\n",
    "reasoning_flag = False\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "        num_dists=3,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=50,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning Train: Average Reward: 0.81 ± 0.26\n",
    "# Reasoning Test: Average Reward: 0.76 ± 0.32\n",
    "# Non-Reasoning Train: Average Reward: 0.74 ± 0.33\n",
    "# Non-Reasoning Test: Average Reward: 0.70 ± 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: BabyAI-Pickup-v0 with Reasoning (base model)\n",
    "# Sample size: 10\n",
    "# Success Rate: 0.50\n",
    "# Average Reward: 0.37 ± 0.40\n",
    "# Average Episode Length: 14.00 ± 9.76\n",
    "# Average Number of Invalid Actions: 3.60\n",
    "\n",
    "# Summary: BabyAI-Pickup-v0 with Reasoning (finetuned model but not run until convergence)\n",
    "# Sample size: 10\n",
    "# Success Rate: 0.70\n",
    "# Average Reward: 0.56 ± 0.41\n",
    "# Average Episode Length: 15.10 ± 10.79\n",
    "# Average Number of Invalid Actions: 2.50\n",
    "\n",
    "# Summary: BabyAI-Pickup-v0 with Reasoning (finetuned for longer)\n",
    "# Sample size: 12\n",
    "# Success Rate: 0.92\n",
    "# Average Reward: 0.80 ± 0.26\n",
    "# Average Episode Length: 9.33 ± 4.29\n",
    "# Average Number of Invalid Actions: 0.50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
