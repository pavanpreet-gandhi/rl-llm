{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: /home/ubuntu/rl-llm\n",
      "Current directory: /home/ubuntu/rl-llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if we're in the root directory of rl-llm repo\n",
    "if not os.path.basename(os.getcwd()) == 'rl-llm':\n",
    "    # If we're in a subdirectory of rl-llm, find the root and cd to it\n",
    "    current_path = os.getcwd()\n",
    "    while os.path.basename(current_path) != 'rl-llm' and os.path.dirname(current_path) != current_path:\n",
    "        current_path = os.path.dirname(current_path)\n",
    "    \n",
    "    if os.path.basename(current_path) == 'rl-llm':\n",
    "        os.chdir(current_path)\n",
    "        print(f\"Changed directory to: {current_path}\")\n",
    "    else:\n",
    "        print(\"Not in rl-llm repository structure\")\n",
    "else:\n",
    "    print(\"Already in rl-llm root directory\")\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-GoTo-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-Open-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-Pickup-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-PutNext-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "2025-04-12 00:25:07.498599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744417507.508218   11819 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744417507.513380   11819 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from src import EnvManager, sample_episodes\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from peft import PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850e15f1866d42b6be1b1d522c542b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8419c4a54174ede8033d12be2162b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89426fd8fce415aaa402e4114a3cb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/325 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76a40205cb94eb48229cb4038c8b80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/788 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Loading with PEFT adapters from: pavanpreet-gandhi/babyai-classical-ppo-prefinal-experiments-mix_hard_reason_early_stopping_1_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80e6cc623d945bbb8233637350d11b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-3B-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1388b8ceb83348829448e3e76be4e7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3937b0262c1e4568b5333f32e978af6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a319b5a43b9545c8a86c918e2591c8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output:\n",
      "You are in a room with a key. The instruction is to: pick up the key.\n",
      "\n",
      "You are not allowed to carry the key with you. You are allowed to carry a small bag or backpack.\n",
      "\n",
      "So, I will put the key on a table, but I will also put another key on the table. And you will have to\n"
     ]
    }
   ],
   "source": [
    "def load_model_with_peft_and_vhead(model_id, revision=None):\n",
    "    \"\"\"\n",
    "    Load a model with both PEFT adapters and value head\n",
    "    \n",
    "    Args:\n",
    "        model_id: Path to model or HF hub model ID\n",
    "        device: Device to load the model to\n",
    "        revision: Specific model revision/commit hash to load\n",
    "    \n",
    "    Returns:\n",
    "        model: The loaded model with adapters and value head\n",
    "        tokenizer: The associated tokenizer\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left', revision=revision)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Get PEFT config to find base model\n",
    "    peft_config = PeftConfig.from_pretrained(model_id, revision=revision)\n",
    "    base_model_id = peft_config.base_model_name_or_path\n",
    "    \n",
    "    print(f\"Base model: {base_model_id}\")\n",
    "    print(f\"Loading with PEFT adapters from: {model_id}\")\n",
    "    if revision:\n",
    "        print(f\"Using revision: {revision}\")\n",
    "    \n",
    "    # Initialize model with value head from the base model\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        base_model_id,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Now load the PEFT adapter weights\n",
    "    model = model.from_pretrained(model_id, device_map=\"auto\", revision=revision)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_base_model_with_vhead(model_id):\n",
    "    \"\"\"\n",
    "    Load a base model with value head\n",
    "    \n",
    "    Args:\n",
    "        model_id: Path to model or HF hub model ID\n",
    "    \n",
    "    Returns:\n",
    "        model: The loaded model with value head\n",
    "        tokenizer: The associated tokenizer\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Loading base model with value head from: {model_id}\")\n",
    "    \n",
    "    # Initialize model with value head\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model_id = \"pavanpreet-gandhi/babyai-classical-ppo-prefinal-experiments-2025-04-11_13-38-03\"\n",
    "model_id = \"pavanpreet-gandhi/babyai-classical-ppo-prefinal-experiments-mix_hard_reason_early_stopping_1_1\"\n",
    "revision = None # \"0eb2d6751c2ba48a0d95302a10ee84f1b4ab274b\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, tokenizer = load_model_with_peft_and_vhead(model_id, revision)\n",
    "# model, tokenizer = load_base_model_with_vhead(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Test the model with a simple query\n",
    "prompt = \"You are in a room with a key. The instruction is to: pick up the key\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "print(\"\\nModel output:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 \n",
      "Summary: BabyAI-Pickup-v0 with Reasoning\n",
      "Sample size: 12\n",
      "Success Rate: 0.92\n",
      "Average Reward: 0.80 ± 0.26\n",
      "Average Episode Length: 9.33 ± 4.29\n",
      "Average Number of Invalid Actions: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.8,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "env_id = \"BabyAI-Pickup-v0\"\n",
    "reasoning_flag = True\n",
    "env_ids = [env_id]\n",
    "num_envs = 6\n",
    "env_managers = [\n",
    "    EnvManager(\n",
    "        env_ids, \n",
    "        invalid_action_penalty=-2,\n",
    "        consecutive_invalid_actions_allowed=5,\n",
    "        reasoning_flag=reasoning_flag,\n",
    "    )\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "\n",
    "stats, contexts = sample_episodes(\n",
    "    envs=env_managers,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    generation_kwargs=generation_kwargs,\n",
    "    device=device,\n",
    "    number_of_episodes=10,\n",
    "    context_window=5,\n",
    "    reasoning_flag=reasoning_flag,\n",
    ")\n",
    "success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "print(f\"Sample size: {len(stats['success'])}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: BabyAI-Pickup-v0 with Reasoning (base model)\n",
    "# Sample size: 10\n",
    "# Success Rate: 0.50\n",
    "# Average Reward: 0.37 ± 0.40\n",
    "# Average Episode Length: 14.00 ± 9.76\n",
    "# Average Number of Invalid Actions: 3.60\n",
    "\n",
    "# Summary: BabyAI-Pickup-v0 with Reasoning (finetuned model but not run until convergence)\n",
    "# Sample size: 10\n",
    "# Success Rate: 0.70\n",
    "# Average Reward: 0.56 ± 0.41\n",
    "# Average Episode Length: 15.10 ± 10.79\n",
    "# Average Number of Invalid Actions: 2.50\n",
    "\n",
    "# Summary: BabyAI-Pickup-v0 with Reasoning (finetuned for longer)\n",
    "# Sample size: 12\n",
    "# Success Rate: 0.92\n",
    "# Average Reward: 0.80 ± 0.26\n",
    "# Average Episode Length: 9.33 ± 4.29\n",
    "# Average Number of Invalid Actions: 0.50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
