{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: /home/ubuntu/rl-llm\n",
      "Current directory: /home/ubuntu/rl-llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if we're in the root directory of rl-llm repo\n",
    "if not os.path.basename(os.getcwd()) == 'rl-llm':\n",
    "    # If we're in a subdirectory of rl-llm, find the root and cd to it\n",
    "    current_path = os.getcwd()\n",
    "    while os.path.basename(current_path) != 'rl-llm' and os.path.dirname(current_path) != current_path:\n",
    "        current_path = os.path.dirname(current_path)\n",
    "    \n",
    "    if os.path.basename(current_path) == 'rl-llm':\n",
    "        os.chdir(current_path)\n",
    "        print(f\"Changed directory to: {current_path}\")\n",
    "    else:\n",
    "        print(\"Not in rl-llm repository structure\")\n",
    "else:\n",
    "    print(\"Already in rl-llm root directory\")\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-GoTo-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-Open-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-Pickup-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ubuntu/rl-llm/.venv/lib/python3.10/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment BabyAI-PutNext-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "2025-04-09 18:51:03.465710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744224663.475225    9961 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744224663.480311    9961 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from src import EnvManager, sample_episodes\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a05ab6681c41f782bacfadf40e7726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"pavanpreet-gandhi/babyai-classical-ppo-prefinal-experiments-mix_hard_reason_early_stopping_1_1\"\n",
    "peft_config = PeftConfig.from_pretrained(model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path,device_map=\"auto\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_id)\n",
    "model = peft_model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.8,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: BabyAI-GoTo-v0 with No Reasoning\n",
      "Sample size: 20\n",
      "Success Rate: 1.00\n",
      "Average Reward: 0.91 ± 0.05\n",
      "Average Episode Length: 6.70 ± 3.81\n",
      "Average Number of Invalid Actions: 0.10\n",
      "\n",
      "Summary: BabyAI-GoTo-v0 with Reasoning\n",
      "Sample size: 23\n",
      "Success Rate: 0.13\n",
      "Average Reward: 0.12 ± 0.33\n",
      "Average Episode Length: 5.09 ± 0.79\n",
      "Average Number of Invalid Actions: 4.70\n",
      "\n",
      "Summary: BabyAI-Pickup-v0 with No Reasoning\n",
      "Sample size: 20\n",
      "Success Rate: 1.00\n",
      "Average Reward: 0.88 ± 0.05\n",
      "Average Episode Length: 8.60 ± 4.13\n",
      "Average Number of Invalid Actions: 0.25\n",
      "\n",
      "Summary: BabyAI-Pickup-v0 with Reasoning\n",
      "Sample size: 20\n",
      "Success Rate: 0.05\n",
      "Average Reward: 0.04 ± 0.20\n",
      "Average Episode Length: 5.70 ± 1.69\n",
      "Average Number of Invalid Actions: 4.90\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9961/2836229921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         stats = sample_episodes(\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0menvs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_managers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl-llm/src/sample_episodes.py\u001b[0m in \u001b[0;36msample_episodes\u001b[0;34m(envs, tokenizer, model, generation_kwargs, device, number_of_episodes, context_window, reasoning_flag)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Generate responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         output = model.generate(\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl-llm/.venv/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl-llm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2464\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl-llm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3418\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3420\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3421\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3422\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl-llm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2614\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2615\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for env_id in [\"BabyAI-GoTo-v0\", \"BabyAI-Pickup-v0\", \"BabyAI-PutNext-v0\", \"BabyAI-Open-v0\", \"BabyAI-PickUpSeqGoTo\"]:\n",
    "\n",
    "    for reasoning_flag in [False, True]:\n",
    "\n",
    "        env_ids = [env_id]\n",
    "        num_envs = 6\n",
    "        env_managers = [\n",
    "            EnvManager(\n",
    "                env_ids, \n",
    "                invalid_action_penalty=-2,\n",
    "                consecutive_invalid_actions_allowed=5,\n",
    "            )\n",
    "            for i in range(num_envs)\n",
    "        ]\n",
    "        reasoning_flag = reasoning_flag\n",
    "\n",
    "        start_time = time.time()\n",
    "        stats = sample_episodes(\n",
    "            envs=env_managers,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            generation_kwargs=generation_kwargs,\n",
    "            device=device,\n",
    "            number_of_episodes=20,\n",
    "            context_window=5,\n",
    "            reasoning_flag=reasoning_flag,\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        success_rate = sum(stats[\"success\"]) / len(stats[\"success\"])\n",
    "        avg_reward = sum(stats[\"rewards\"]) / len(stats[\"rewards\"])\n",
    "        std_reward = torch.std(torch.tensor(stats[\"rewards\"], dtype=torch.float32)).item()\n",
    "        avg_episode_length = sum(stats[\"episode_lengths\"]) / len(stats[\"episode_lengths\"])\n",
    "        std_episode_length = torch.std(torch.tensor(stats[\"episode_lengths\"], dtype=torch.float32)).item()\n",
    "        avg_num_invalid_actions = sum(stats[\"num_invalid_actions\"]) / len(stats[\"num_invalid_actions\"])\n",
    "\n",
    "        print(f\"Summary: {env_id} with {'Reasoning' if reasoning_flag else 'No Reasoning'}\")\n",
    "        print(f\"Sample size: {len(stats['success'])}\")\n",
    "        print(f\"Success Rate: {success_rate:.2f}\")\n",
    "        print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "        print(f\"Average Episode Length: {avg_episode_length:.2f} ± {std_episode_length:.2f}\")\n",
    "        print(f\"Average Number of Invalid Actions: {avg_num_invalid_actions:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model (no finetuning)\n",
    "\n",
    "Summary: BabyAI-GoTo-v0 with No Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.65\n",
    "Average Reward: 0.51 ± 0.40\n",
    "Average Episode Length: 23.10 ± 17.94\n",
    "Average Number of Invalid Actions: 2.15\n",
    "\n",
    "Summary: BabyAI-GoTo-v0 with Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.95\n",
    "Average Reward: 0.77 ± 0.22\n",
    "Average Episode Length: 14.80 ± 8.84\n",
    "Average Number of Invalid Actions: 1.60\n",
    "\n",
    "Summary: BabyAI-Pickup-v0 with No Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.55\n",
    "Average Reward: 0.43 ± 0.41\n",
    "Average Episode Length: 28.65 ± 21.20\n",
    "Average Number of Invalid Actions: 2.30\n",
    "\n",
    "Summary: BabyAI-Pickup-v0 with Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.70\n",
    "Average Reward: 0.47 ± 0.36\n",
    "Average Episode Length: 21.30 ± 15.40\n",
    "Average Number of Invalid Actions: 2.50\n",
    "\n",
    "Summary: BabyAI-PutNext-v0 with No Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.05\n",
    "Average Reward: 0.01 ± 0.04\n",
    "Average Episode Length: 78.90 ± 48.60\n",
    "Average Number of Invalid Actions: 3.65\n",
    "\n",
    "Summary: BabyAI-PutNext-v0 with Reasoning\n",
    "Sample size: 21\n",
    "Success Rate: 0.19\n",
    "Average Reward: 0.07 ± 0.17\n",
    "Average Episode Length: 95.81 ± 50.48\n",
    "Average Number of Invalid Actions: 2.57\n",
    "\n",
    "Summary: BabyAI-Open-v0 with No Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.35\n",
    "Average Reward: 0.16 ± 0.25\n",
    "Average Episode Length: 79.35 ± 34.19\n",
    "Average Number of Invalid Actions: 3.50\n",
    "\n",
    "Summary: BabyAI-Open-v0 with Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.30\n",
    "Average Reward: 0.15 ± 0.25\n",
    "Average Episode Length: 81.65 ± 48.86\n",
    "Average Number of Invalid Actions: 3.05\n",
    "\n",
    "Summary: BabyAI-PickUpSeqGoTo with No Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.55\n",
    "Average Reward: 0.32 ± 0.32\n",
    "Average Episode Length: 83.40 ± 39.94\n",
    "Average Number of Invalid Actions: 0.90\n",
    "\n",
    "Summary: BabyAI-PickUpSeqGoTo with Reasoning\n",
    "Sample size: 20\n",
    "Success Rate: 0.35\n",
    "Average Reward: 0.19 ± 0.32\n",
    "Average Episode Length: 84.50 ± 49.86\n",
    "Average Number of Invalid Actions: 2.90\n",
    "\n",
    "\n",
    "## Finetuned model (allowed to reason)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
