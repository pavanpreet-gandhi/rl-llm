{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if not in root dir, change to root dir\n",
    "if os.path.basename(os.getcwd()) != \"rl-llm\":\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import gym, babyai_text\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/rl-llm/.conda/lib/python3.11/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'HuggingFaceTB/SmolLM2-135M-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "/home/pavan/rl-llm/.conda/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "/home/pavan/rl-llm/.conda/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:273: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = PPOConfig(batch_size=4, mini_batch_size=4)\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)\n",
    "ref_model = create_reference_model(model)\n",
    "\n",
    "trainer = PPOTrainer(config, model, ref_model, tokenizer)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 10,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager:\n",
    "\n",
    "    def __init__(self, env: gym.Env, invalid_action_penalty: float = -0.1, consecutive_invalid_actions_allowed: int = 5):\n",
    "        self.env = env\n",
    "        self.invalid_action_penalty = invalid_action_penalty\n",
    "        self.consecutive_invalid_actions_allowed = consecutive_invalid_actions_allowed\n",
    "        self.consecutive_invalid_actions = 0\n",
    "    \n",
    "    def reset(self) -> Tuple[str, str]:\n",
    "        self.consecutive_invalid_actions = 0\n",
    "        obs, info = self.env.reset()\n",
    "        mission = obs[\"mission\"]\n",
    "        text_obs = \"\\n\".join(info[\"descriptions\"])\n",
    "        return mission, text_obs\n",
    "    \n",
    "    def step(self, text_action: str) -> Tuple[str, float, bool]:\n",
    "        action = utils.text_to_action.get(text_action, None)\n",
    "        if action is None:\n",
    "            self.consecutive_invalid_actions += 1\n",
    "            text_obs = \"You entered an invalid action, the valid actions are: \" + str(list(utils.text_to_action.keys()))\n",
    "            reward = self.invalid_action_penalty\n",
    "            done = self.consecutive_invalid_actions >= self.consecutive_invalid_actions_allowed\n",
    "        else:\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            text_obs = \"\\n\".join(info[\"descriptions\"])\n",
    "        return text_obs, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/rl-llm/.conda/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/pavan/rl-llm/.conda/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method should be an int or np.int64, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_tensors: [torch.Size([347]), torch.Size([347]), torch.Size([347]), torch.Size([347])]\n",
      "response_tensors: [torch.Size([20]), torch.Size([20]), torch.Size([20]), torch.Size([20])]\n",
      "action_texts: ['To the right, 3 steps right,\\nTo the left, 4 steps left,\\n', 'I see you are trying to interact with the key and the ball, and it seems like you are', 'You see a purple key 1 step left, 1 step forward. You see a grey box', \"The key to picking up a puzzle is using the 'toggle' action to interact with a certain\"]\n"
     ]
    }
   ],
   "source": [
    "num_envs = 4\n",
    "experiences_needed = 10\n",
    "\n",
    "query_tensors_all, response_tensors_all, rewards_all = [], [], [] # list of all experiences we have collected so far\n",
    "\n",
    "envs = [EnvManager(gym.make(\"BabyAI-MixedTrainLocal-v0\", seed=i)) for i in range(num_envs)]\n",
    "missions, text_obss = zip(*[env.reset() for env in envs])\n",
    "\n",
    "messagess = [[] for _ in range(num_envs)]\n",
    "rewardss = [[] for _ in range(num_envs)]\n",
    "system_prompt_template = utils.get_system_prompt()\n",
    "for messages, mission, text_obs in zip(messagess, missions, text_obss):\n",
    "    system_prompt = system_prompt_template.replace(\"{goal}\", mission)\n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": text_obs})\n",
    "\n",
    "while len(rewards_all) < experiences_needed:\n",
    "\n",
    "    query_tensors = tokenizer.apply_chat_template(\n",
    "        messagess, \n",
    "        return_tensors=\"pt\", \n",
    "        add_generation_prompt=True, \n",
    "        padding=\"longest\",\n",
    "        padding_side=\"left\",\n",
    "    ).to(device)\n",
    "    query_tensors = [tensor for tensor in query_tensors]\n",
    "    print(f\"query_tensors: {[tensor.shape for tensor in query_tensors]}\")\n",
    "\n",
    "    response_tensors = trainer.generate(query_tensors, **generation_kwargs, return_prompt=False)\n",
    "    print(f\"response_tensors: {[tensor.shape for tensor in response_tensors]}\")\n",
    "\n",
    "    action_texts = tokenizer.batch_decode(\n",
    "        response_tensors, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    for messages, action_text in zip(messagess, action_texts):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": action_text})\n",
    "    print(f\"action_texts: {action_texts}\")\n",
    "\n",
    "    break\n",
    "\n",
    "    # step the envs\n",
    "    # for each env take a step\n",
    "    # if the episode is done:\n",
    "    # - add the final reward to all previous rewards\n",
    "    # - append the query_tensors, response_tensors, and rewards from that episode to the list of all experiences that we have collected so far\n",
    "    # - reset the env(s) that was done\n",
    "    # - keep collecting experiences until we have enough\n",
    "\n",
    "    # Thought dump:\n",
    "    # NOTE 1: Each time an episode ends, we add all the experiences from that episode to the list of all experiences we have collected so far\n",
    "    # This means that we could end up with more than experiences_needed experiences in the list.\n",
    "    # if that happends we just take the first experiences_needed experiences from the list\n",
    "    # NOTE 2: There is no need for multiprocessing since the overhead from the environment is not that much. \n",
    "    # All we need to do for now is use batch inference with the LLM so that each forward pass of the LLM allows us to collect num_processes experiences\n",
    "    # (that get appended once the episode ends)\n",
    "    # NOTE 3: The resulting function should be a drop in replacement to the while loop in train.py. \n",
    "    # The only additional changes are probably just in setup_training.py which would setup multiple environments (and possibly env managers for convenience instead of one)\n",
    "    # NOTE 4: Different envs could be done at different times, even though while debugging with SmolLM2-135M they will all probably end at the same time due to invalid actions.\n",
    "    # NOTE 5: The root dir can be treated as a module. From the root dir you can run pip install -e . to install the package in editable mode, \n",
    "    # so we don't need to sys.path.append every time we want to import something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected function signature\n",
    "def collect_trajectories_batched(\n",
    "    envs: List[EnvManager], \n",
    "    trainer: PPOTrainer, \n",
    "    tokenizer: PreTrainedTokenizer, \n",
    "    generation_kwargs: Dict[str, Any], \n",
    "    experiences_needed: int,\n",
    "    consecutive_invalid_actions_allowed: int = 5,\n",
    "    invalid_action_penalty: float = -0.1,\n",
    "    ) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
